#!/usr/bin/env python2.7
import sys, os, urllib2, argparse, re, cgi, textwrap
from xml.etree import ElementTree as ET

def parsePubmed(doc, id):
    infoDict = dict()
    infoDict['url'] = "http://www.ncbi.nlm.nih.gov/pubmed/%s" % id
    attribList = ['PubDate', 'Source', 'Title', 'Volume', 'Issue', 'Pages', 'SO', 'CollectiveName']
    for element in doc:
        if element.tag != "DocSum":
            continue
        items = element.findall("Item")
        for i in items:
            if i.attrib['Name'] == 'AuthorList':
                infoDict['Authors'] = list()
                for j in i:
                    infoDict['Authors'].append(j.text)
                continue
            if i.attrib['Name'] in attribList:
                infoDict[i.attrib['Name']] = i.text

    return infoDict

def parsePmc(doc, id):
    infoDict = dict()
    infoDict['url'] = "http://www.ncbi.nlm.nih.gov/pmc/articles/%s" % id
    attribList = ['Title', 'PubDate', 'Journal', 'Volume', 'Issue', 'Pagination', 'CollectiveName']
    for node in doc.iter("ArticleId"):
        foundPubMedId = 0
        for child in node:
            if child.tag == "IdType" and child.text == "pmid":
                foundPubMedId = 1
            if foundPubMedId == 1 and child.tag == "Value":
                return parseInit(child.text) 
            
    sys.stderr.write("Unable to find pubmed id for pubmed central id: %s\n" % id)
    sys.exit()

def parseInit(id):
    urlbase = "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?"
    db = "Pubmed"
    url = urlbase + "db=%s" % db + "&id=%s" % id
    if re.match("^PMC", id):
        db = "PMC"
        id = re.sub("PMC", "", id)
        url = urlbase + "db=%s" % db + "&id=%s&version=2.0" % id
    sys.stderr.write("Accessing %s\n" % url)
    fetch = urllib2.urlopen(url)
    #sidefetch = urllib2.urlopen(url)
    #print "%s" % sidefetch.read()

    doc = ET.XML(fetch.read())
    if db == "Pubmed":
        infoDict = parsePubmed(doc, id)
    elif db == "PMC":
        infoDict = parsePmc(doc, id)

    return infoDict

def htmlEscape(str):
    return cgi.escape(str).encode('ascii', 'xmlcharrefreplace')

def makeHtml(infoDict, original, plain):
    authors = list()
    authcount = 0
    etal = 0
    if 'CollectiveName' in infoDict:
        authors.append(infoDict['CollectiveName'])
        authcount = 1
    for i in infoDict['Authors']:
        if authcount == 10:
            etal = 1
            break
        authors.append(i)
        authcount = authcount + 1
    sep = ", "
    authStr = sep.join(authors)
    authStr = htmlEscape(authStr)
    if etal and not plain:
        authStr = authStr + " <em>et al</em>"
    if etal and plain:
       authStr = authStr + " et al"
    authStr = authStr + "."
    title = re.sub("\.$", "", infoDict['Title'])
    if 'Source' in infoDict:
        journal = infoDict['Source']
    elif 'Journal' in infoDict:
        journal = infoDict['Journal']
    if 'SO' in infoDict:
        dateStr = re.sub(";:", ":", infoDict['SO'])
    else:
        dateStr1 = infoDict['PubDate']
        dateStr = ""
        if 'Volume' in infoDict:
            dateStr = dateStr + ";%s" % infoDict['Volume']
        if 'Issue' in infoDict:
            dateStr = dateStr + "(%s)" % infoDict['Issue']
        if 'Pagination' in infoDict:
            dateStr = dateStr + "%s" % infoDict['Pagination']
        elif 'Pages' in infoDict:
            dateStr = dateStr + ":%s" %infoDict['Pages']
        dateStr = re.sub("\s+","", dateStr)
        dateStr = dateStr1 + dateStr
    if not re.search("\.$", dateStr):
        dateStr = dateStr + "."
    origComment = ""
    if original:
        fetch = urllib2.urlopen(infoDict['url'])
        doc = fetch.read()
        m = re.search('<div class="icons"><a href="(\S+)"', doc)
        if m:
            if m.group(1):
                infoDict['url'] = m.group(1).replace("&amp;", "&")
        else:
            origComment = "<!-- Can't find original article link for %s -->" % infoDict['url']
    htmlLines = list()
    htmlLines.append("<p>")
    htmlLines.append("%s" % authStr)
    htmlLines.append("<a href=\"%s\" target=\"_blank\">" % htmlEscape(infoDict['url']))
    htmlLines.append("%s</a>." % htmlEscape(title))
    htmlLines.append("<em>%s</em>. %s" % (htmlEscape(journal), dateStr))
    htmlLines.append("</p>")
    if plain:
        htmlLines = list()
        htmlLines.append("%s %s. %s. %s %s" % (authStr, title, journal, dateStr, infoDict['url']))
    if not plain and origComment:
        htmlLines.append("%s" %  origComment)
    sep = "\n"
    space = " "

    processLines = list()

    for i in htmlLines:
        processLines.append(textwrap.fill(i, 100))
    htmlStr = sep.join(processLines)
    return htmlStr, authStr

def main():
    parser = argparse.ArgumentParser(
        prog='getTrackReferences',
        description='Turns PubMed Ids and PubMedCentral Ids into GB formatted citations in html',
        epilog='example: getTrackReferences PMC3039671 21347206'
        )
    parser.add_argument('ids', metavar='IDs', type=str, nargs='+', help='The list of PubMed and PubMedCentral Ids')
    parser.add_argument('-o', '--original', action="store_true", default=0, help="Use original article link instead of PubMed link. Currently an experimental feature.")
    parser.add_argument('-p', '--plain', action="store_true", default=0, help="Output the references in plain-text instead of html.")
    #if len(sys.argv) <= 1:
    #    parser.print_help()
    args = parser.parse_args(sys.argv[1:])
    ids = args.ids
    references = dict()

    for i in ids:
        infoDict = parseInit(i)
        html, authStr = makeHtml(infoDict, args.original, args.plain)
        references[authStr] = html
    print "\n"
    for i in sorted(references.keys()):
        print "%s\n" % references[i]

if __name__ == "__main__":
    main()
